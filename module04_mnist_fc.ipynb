{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlameDanita/Neural_Networks_and_CV/blob/master/module04_mnist_fc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DWr6cvb9pS3J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MqGQWTDIpS3R"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets\n",
        "MNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)\n",
        "MNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv_Lz7PYpS3U",
        "outputId": "f2b9896a-bc96-4857-db75-13e38180b46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ],
      "source": [
        "X_train = MNIST_train.train_data\n",
        "y_train = MNIST_train.train_labels\n",
        "X_test = MNIST_test.test_data\n",
        "y_test = MNIST_test.test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMhsAedlrQF5",
        "outputId": "676e7978-1ea0-436a-a245-b2ac4db6434b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.uint8, torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X_train.dtype, y_train.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_yTaVOrPvap6"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.float()\n",
        "X_test = X_test.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gfupg4kpS3X",
        "outputId": "b2877570-ab70-45d5-9414-0de0354e2988"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEC1RXBEz_SW",
        "outputId": "84646ee0-db76-4597-e981-52aeb43a41ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000]), torch.Size([10000]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Z1tFXMwJpS3e",
        "outputId": "a9f83079-a100-4a00-f18f-aa6e7ad09603"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(X_train[0, :, :])\n",
        "plt.show()\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IUZgNg7zpS3j"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape([-1, 28 * 28])\n",
        "X_test = X_test.reshape([-1, 28 * 28])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k7l65slppS3l"
      },
      "outputs": [],
      "source": [
        "class MNISTNet(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_neurons):\n",
        "        super(MNISTNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n",
        "        self.ac1 = torch.nn.Sigmoid()\n",
        "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.ac1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "mnist_net = MNISTNet(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJp7t3dR0pC5",
        "outputId": "d676ac8f-0fdb-44a6-ac1e-66ea80e02146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShQDv_w-0rz8",
        "outputId": "4ffe6d1d-547e-477a-ff87-4745be37a932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 12 23:04:56 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    30W /  70W |      3MiB / 15360MiB |      5%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUHy-780po9d",
        "outputId": "b8bdd29b-8455-4649-f71e-dc68b1b49d45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
              "         [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
              "         [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
              "         ...,\n",
              "         [ 0.0221,  0.0258, -0.0088,  ..., -0.0141,  0.0051, -0.0318],\n",
              "         [-0.0217, -0.0136,  0.0185,  ..., -0.0012, -0.0012, -0.0017],\n",
              "         [ 0.0142,  0.0089, -0.0053,  ...,  0.0311, -0.0181,  0.0020]],\n",
              "        device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([-3.5402e-02,  1.8178e-02, -2.1709e-02, -1.1839e-02,  4.8722e-03,\n",
              "         -1.0492e-02, -1.9008e-02,  2.6994e-02, -3.4899e-02,  1.0381e-02,\n",
              "         -3.5228e-02, -5.6381e-03, -2.0134e-02, -3.0264e-02, -1.3289e-02,\n",
              "          2.5346e-02, -1.0797e-02,  2.1568e-03,  4.3630e-03, -2.1916e-02,\n",
              "          2.2670e-02, -1.5766e-02,  3.4906e-02, -2.3063e-02, -6.8216e-03,\n",
              "          2.7051e-02, -1.3991e-02,  9.7218e-03, -9.9075e-04,  1.6625e-03,\n",
              "          2.3455e-02, -2.5294e-02,  4.8024e-06, -2.5339e-02, -1.7412e-02,\n",
              "         -1.6342e-02,  2.6164e-03, -1.8025e-02,  2.5939e-02,  2.3884e-03,\n",
              "          1.1857e-03, -2.0811e-03, -1.4417e-03,  2.2246e-02, -2.7703e-02,\n",
              "          1.9103e-02,  1.7750e-02, -4.5987e-03, -5.7228e-03,  1.1429e-02,\n",
              "         -1.9616e-02,  1.0336e-04,  2.3019e-02, -1.9994e-02,  1.5292e-03,\n",
              "         -3.4574e-02, -2.8799e-02,  1.0611e-02, -1.4141e-02, -2.3886e-02,\n",
              "          2.2307e-02, -3.0040e-02, -2.8897e-02, -9.2013e-03, -3.6146e-03,\n",
              "          3.4074e-03,  7.3265e-03, -1.7886e-02, -1.8164e-02, -1.9736e-02,\n",
              "         -3.0381e-02, -1.5204e-02,  2.4328e-02, -2.2534e-02,  4.2453e-04,\n",
              "          2.7948e-02,  1.3828e-03,  3.3543e-02, -3.3373e-02, -6.0236e-04,\n",
              "          1.1359e-02,  3.2776e-02, -3.6375e-03, -2.7758e-02, -4.0390e-03,\n",
              "         -1.8094e-03, -2.1388e-02,  1.8606e-03, -1.4006e-02, -2.9517e-02,\n",
              "         -1.6692e-02, -7.7352e-03, -1.9986e-02,  1.8643e-02, -2.0421e-02,\n",
              "          1.1466e-02, -2.7346e-02,  2.3264e-03, -3.3796e-02,  1.1720e-02],\n",
              "        device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([[-4.4030e-02, -4.0902e-02, -2.7658e-03, -7.5691e-02,  4.7223e-02,\n",
              "           2.0091e-02,  5.2066e-03,  2.9781e-02,  2.0557e-02, -3.9398e-02,\n",
              "          -4.3418e-02, -4.2620e-02,  2.0538e-02, -6.9487e-02, -1.0267e-03,\n",
              "           6.5385e-02, -2.4658e-03,  8.2882e-02,  5.6582e-02, -9.1259e-02,\n",
              "          -8.1402e-02,  9.4457e-02,  4.5791e-02, -9.5087e-02,  1.7097e-02,\n",
              "           4.3891e-02,  3.8794e-02,  2.8788e-02,  2.2401e-02, -6.5288e-02,\n",
              "           4.3317e-02,  2.0658e-02, -8.5607e-02, -5.5555e-02,  6.7581e-02,\n",
              "          -3.3979e-03,  9.5181e-02,  5.3261e-02, -5.3859e-02,  7.1806e-03,\n",
              "          -8.9716e-02, -5.7855e-02, -9.5984e-02, -8.0224e-02, -3.1562e-03,\n",
              "          -6.7034e-02, -9.0357e-02, -7.6925e-02, -8.4575e-02,  3.1875e-02,\n",
              "          -3.9122e-02,  2.7084e-02, -3.2642e-02, -3.9497e-02, -7.0178e-03,\n",
              "          -1.7398e-02,  5.8693e-02, -5.4878e-02,  4.2710e-02, -7.1184e-02,\n",
              "          -2.8224e-02, -4.4826e-02,  1.3082e-02,  4.7396e-02,  4.2165e-02,\n",
              "          -1.4485e-02,  6.3214e-02, -1.7163e-02, -5.6876e-02,  7.2040e-02,\n",
              "           4.9951e-02,  1.3729e-02,  1.6482e-02,  6.9580e-02, -8.0455e-03,\n",
              "           2.9021e-02,  5.1953e-03, -8.5754e-02, -7.1343e-02, -9.8854e-02,\n",
              "          -1.2864e-02, -8.1402e-02, -8.8800e-02, -7.0611e-02, -8.5149e-02,\n",
              "           2.2577e-02, -1.7917e-02,  3.4979e-02,  3.6967e-02,  2.0696e-03,\n",
              "          -4.5241e-02, -5.2543e-02,  8.7695e-02, -9.6897e-02, -4.1911e-02,\n",
              "           8.3253e-02,  1.9033e-02,  8.5772e-02, -9.1113e-02, -4.6074e-02],\n",
              "         [ 2.8562e-02, -9.7789e-02,  9.9616e-02,  1.0313e-02,  1.8937e-03,\n",
              "          -2.8054e-02,  5.7647e-02,  7.5185e-03, -7.6182e-02,  4.7473e-02,\n",
              "           9.2353e-02,  8.7539e-02,  4.3655e-02, -8.8865e-02,  5.0226e-02,\n",
              "           2.9909e-02, -9.6574e-02, -3.5466e-02, -9.3904e-02, -4.4798e-02,\n",
              "          -3.3621e-02, -6.5826e-02,  3.6287e-02, -3.3913e-02, -8.2384e-02,\n",
              "          -8.0630e-02, -3.6251e-02,  3.2257e-02, -1.7082e-02,  6.7202e-03,\n",
              "           4.9823e-02,  8.7775e-02, -5.6468e-02,  9.9253e-02,  9.1745e-02,\n",
              "           6.8144e-02,  6.4052e-02,  4.7919e-02, -9.1159e-02,  6.4923e-02,\n",
              "           6.1755e-04,  2.4259e-02, -5.2861e-02, -8.0466e-02, -7.4763e-02,\n",
              "          -2.4290e-02,  8.2725e-02, -5.2735e-02, -1.6988e-02,  8.4985e-03,\n",
              "          -6.6479e-02, -6.3317e-02,  2.7013e-02,  7.6525e-02, -9.6293e-02,\n",
              "          -2.3882e-02, -2.4638e-02, -1.5677e-02,  8.6723e-02,  1.1229e-02,\n",
              "          -3.8456e-02, -9.1262e-03, -1.0519e-02, -9.5751e-02, -1.0230e-02,\n",
              "           2.0495e-02,  1.0166e-02,  1.6618e-02, -7.6543e-02, -1.9185e-02,\n",
              "           3.6591e-03, -4.8461e-02, -3.9484e-02,  4.6251e-02,  6.8032e-02,\n",
              "          -6.6524e-02, -1.0022e-02, -4.8116e-02, -9.9494e-02, -7.2708e-02,\n",
              "           4.8695e-02, -5.4311e-02,  5.3059e-02, -5.1349e-02, -5.3362e-02,\n",
              "           5.6112e-04,  5.6536e-02,  7.5848e-02,  8.7638e-02, -2.6691e-02,\n",
              "           5.4360e-02, -9.7876e-02, -2.8115e-03,  7.7884e-03,  7.6152e-03,\n",
              "           2.9289e-02,  5.4671e-02,  6.9147e-02, -2.9804e-03, -5.5532e-02],\n",
              "         [-5.8062e-02, -1.6233e-02,  5.9640e-02,  3.8012e-02, -7.7325e-02,\n",
              "           5.2889e-02,  4.9676e-02, -8.6207e-02,  1.2076e-02, -6.6163e-02,\n",
              "          -8.3880e-02,  1.7198e-02, -3.7479e-02,  8.4232e-02, -5.0198e-02,\n",
              "           2.3431e-02, -2.2310e-02, -6.4360e-02, -2.8851e-02,  3.1953e-02,\n",
              "           3.2718e-02, -1.6766e-02, -9.4292e-02, -1.6340e-02,  4.0565e-02,\n",
              "          -5.5432e-02,  4.2336e-02,  9.1535e-02, -7.2934e-02,  7.0494e-02,\n",
              "          -7.4497e-02,  1.4436e-02, -5.1857e-03,  7.7257e-02,  2.4613e-02,\n",
              "          -2.3095e-02, -4.3948e-03,  1.7421e-02, -3.5985e-02,  1.2784e-02,\n",
              "          -6.0356e-02, -1.7307e-02, -2.0917e-03,  3.3032e-02, -6.1410e-02,\n",
              "           7.9608e-02,  2.7768e-02, -3.4363e-02, -6.1100e-02, -3.1468e-02,\n",
              "          -3.9813e-02,  3.5793e-02,  2.9954e-03,  1.8687e-02, -7.1464e-02,\n",
              "           1.8687e-02,  6.9480e-02,  8.6372e-02, -5.5390e-02,  1.6557e-02,\n",
              "           1.9878e-02,  8.0226e-02, -2.1137e-02,  9.2464e-02, -2.0413e-02,\n",
              "          -9.3492e-02,  9.0216e-02, -9.9643e-03, -4.5676e-02, -7.2762e-02,\n",
              "           9.5185e-02, -3.5671e-02, -5.7421e-02,  9.6054e-02, -1.0582e-02,\n",
              "           3.5247e-02,  4.4829e-04,  6.2995e-02, -7.0383e-02,  5.9155e-02,\n",
              "           9.5140e-02, -6.1451e-02,  7.1062e-02, -3.1055e-02,  6.2909e-02,\n",
              "           9.0056e-02, -5.9558e-02, -6.3497e-02, -7.2795e-02,  6.3794e-02,\n",
              "          -7.1646e-02, -5.9375e-03,  9.2007e-02, -2.3524e-02,  6.7093e-02,\n",
              "          -9.6363e-02,  8.5956e-02, -5.4083e-02,  4.6875e-02, -7.0707e-02],\n",
              "         [-9.9884e-02, -2.0566e-03, -9.2505e-03,  1.2660e-05,  5.7482e-02,\n",
              "           9.8647e-02,  5.9621e-02, -5.6918e-02, -2.3541e-02,  8.5597e-02,\n",
              "           3.2887e-02, -1.2115e-02,  6.3024e-02, -4.1736e-03,  1.5560e-02,\n",
              "          -3.6098e-02, -2.9918e-02,  9.7293e-02, -3.3788e-02,  6.5262e-02,\n",
              "          -6.2560e-03,  8.2679e-02, -3.4145e-02, -4.2508e-02,  9.3023e-02,\n",
              "           1.6028e-02, -7.6950e-02,  6.5384e-02,  6.1429e-02,  7.9440e-02,\n",
              "          -2.2314e-02, -8.0422e-02, -2.0729e-02, -4.6973e-02, -8.5903e-02,\n",
              "           8.3461e-02,  5.4504e-03,  6.4838e-02, -4.3093e-02, -1.4715e-02,\n",
              "           9.2161e-02, -1.6137e-02,  4.7117e-02,  4.8628e-02,  4.7408e-02,\n",
              "          -9.9202e-02, -9.7989e-02,  4.2185e-02,  6.1507e-02,  9.3029e-02,\n",
              "           2.3039e-02, -8.1951e-02,  4.2035e-02,  5.3145e-02,  1.7351e-02,\n",
              "           2.6871e-02,  3.6373e-02, -4.5585e-02, -7.1108e-02, -6.8185e-03,\n",
              "          -8.1368e-02,  4.8340e-02,  1.8260e-02, -7.0928e-02, -3.2509e-02,\n",
              "           3.3479e-02, -8.1458e-02,  7.4082e-03,  7.0904e-02, -8.2055e-02,\n",
              "          -4.5267e-02, -9.7422e-02,  6.2716e-02,  3.1780e-02,  2.6002e-02,\n",
              "           3.5429e-02, -2.8131e-02, -7.5722e-02,  8.6744e-02,  6.9950e-02,\n",
              "          -4.4811e-03, -3.7405e-02,  8.7347e-02,  2.9528e-02,  2.4509e-02,\n",
              "          -7.9858e-02,  1.0490e-03,  8.5361e-02, -6.4963e-03,  8.6767e-02,\n",
              "           1.9172e-03, -4.7197e-02, -9.4302e-02, -1.1154e-04,  4.4796e-02,\n",
              "           9.0297e-02, -4.5391e-02, -8.2952e-03, -8.3752e-04,  1.0406e-02],\n",
              "         [-8.3833e-02, -1.6800e-02, -1.8120e-02,  2.4787e-02,  1.5949e-02,\n",
              "          -3.6587e-02, -4.0931e-02, -2.8362e-02, -6.2916e-02,  9.5074e-04,\n",
              "           3.7527e-02, -7.5627e-02, -9.0116e-02, -4.8229e-02,  4.8008e-02,\n",
              "          -8.3595e-02, -7.4236e-02, -7.7821e-02, -6.1297e-02, -1.0876e-02,\n",
              "          -7.7140e-02,  9.9012e-02, -5.8988e-02,  9.1224e-02,  8.5215e-02,\n",
              "          -6.2058e-02, -9.7728e-02,  6.1797e-02,  8.5920e-02, -4.6782e-02,\n",
              "           4.5797e-02, -3.4127e-02, -2.6734e-02,  5.0654e-02,  8.0230e-02,\n",
              "          -6.8283e-04,  4.4868e-02,  9.9643e-02, -7.7270e-03, -6.9289e-03,\n",
              "           7.8843e-02, -6.2620e-02,  4.6331e-02,  3.5543e-02,  4.4476e-02,\n",
              "           6.4701e-02,  5.5111e-02, -1.3538e-02, -5.7666e-02,  3.1949e-02,\n",
              "          -7.4435e-02, -1.0555e-02, -2.9269e-02,  7.6828e-02,  6.0613e-02,\n",
              "          -1.0246e-02, -7.7053e-02, -8.1615e-02,  7.8583e-02, -3.7298e-02,\n",
              "           9.8287e-02, -8.7990e-02,  8.5202e-02,  7.3779e-02, -7.5497e-02,\n",
              "          -5.0194e-02,  1.7486e-02, -8.1503e-02,  4.3283e-02,  3.1912e-02,\n",
              "           3.3300e-02,  8.4082e-02, -1.5393e-02, -1.0400e-02, -5.7354e-02,\n",
              "           6.4003e-02,  7.5777e-02,  8.7583e-02,  1.0506e-02, -1.0069e-02,\n",
              "           4.6248e-02,  6.8776e-02,  9.1238e-02,  8.2242e-02, -6.3215e-02,\n",
              "          -2.2493e-03,  5.6948e-02, -1.4380e-02,  9.5676e-02,  9.8546e-02,\n",
              "           3.5439e-02, -6.1159e-02, -3.4391e-02,  4.1154e-02, -4.9081e-02,\n",
              "          -8.1823e-02, -1.6583e-02, -7.9054e-02, -8.9295e-02,  6.5986e-02],\n",
              "         [-4.6052e-02, -5.7329e-02, -6.9434e-02,  8.0967e-02, -1.9942e-02,\n",
              "          -5.2907e-02,  3.7663e-03, -6.5540e-02,  7.4881e-03, -7.2165e-02,\n",
              "          -5.0907e-03,  5.9958e-02,  7.3126e-02,  7.8212e-02,  9.5219e-02,\n",
              "          -5.8539e-02, -5.7033e-02,  5.0800e-02,  2.9198e-02,  5.7162e-02,\n",
              "           5.8643e-02,  2.4181e-02,  9.6798e-02, -7.2006e-02, -4.8288e-02,\n",
              "           9.8164e-02, -1.4304e-02,  8.4005e-02, -4.1074e-02,  4.0151e-03,\n",
              "          -2.9527e-02,  8.0418e-02,  4.4058e-02,  1.3534e-02,  2.5140e-02,\n",
              "           2.1768e-02, -9.8031e-02, -6.6325e-02, -3.6608e-02,  2.1682e-02,\n",
              "           8.6108e-02, -5.5307e-02,  2.1730e-02,  2.3178e-02, -2.4123e-02,\n",
              "          -6.6951e-03, -7.9058e-02,  8.1677e-02, -1.8746e-02, -3.1030e-02,\n",
              "           1.0893e-02, -5.3572e-02, -6.3206e-02, -2.8532e-02,  4.3749e-02,\n",
              "          -5.8695e-02,  9.5919e-03, -9.0220e-03,  6.0012e-02, -5.8548e-03,\n",
              "          -1.3501e-02,  7.4022e-02, -9.2908e-02, -1.4278e-02,  5.1928e-02,\n",
              "          -9.1766e-02,  7.3883e-02,  9.7954e-02,  2.0665e-02, -4.6826e-02,\n",
              "           3.8851e-02, -3.2427e-02, -2.0750e-02, -6.1963e-02, -6.8082e-02,\n",
              "           5.1893e-02,  7.8080e-02,  2.3663e-02,  9.9957e-02, -1.9962e-02,\n",
              "          -8.7717e-02, -5.7643e-02,  6.9828e-02,  1.4412e-02,  4.3624e-02,\n",
              "          -6.7853e-02, -8.7193e-02, -7.1548e-02, -5.8304e-02,  9.1487e-02,\n",
              "           8.0201e-02, -3.5304e-02,  5.6775e-02,  8.3264e-02,  6.6919e-02,\n",
              "           3.7993e-02,  5.4683e-03, -3.5126e-02,  6.6001e-03,  2.9616e-02],\n",
              "         [ 9.1658e-02,  9.4709e-02,  1.5736e-02,  2.2302e-02,  4.6048e-02,\n",
              "           1.2184e-02,  4.9628e-03,  7.0882e-02, -1.3095e-03,  6.4723e-02,\n",
              "           7.9142e-02,  1.6253e-02, -1.9196e-02,  4.3337e-02, -5.8832e-02,\n",
              "          -3.2178e-02, -6.3990e-02,  4.7409e-02, -2.7614e-02,  1.7234e-02,\n",
              "          -1.0813e-02, -1.4974e-02,  2.3524e-02,  8.9089e-02, -3.2056e-02,\n",
              "           7.2992e-02, -6.9731e-02,  8.7564e-02, -8.0852e-02, -6.2523e-02,\n",
              "          -7.4421e-02,  4.1387e-02, -7.6536e-02, -4.0783e-02, -6.2073e-02,\n",
              "          -1.3144e-03,  7.2717e-02,  8.8049e-02,  9.7246e-03,  7.4112e-02,\n",
              "          -8.7396e-02,  2.9497e-02,  5.1279e-02, -7.4738e-02, -3.1917e-02,\n",
              "           4.6675e-02, -5.1561e-02, -6.6640e-03,  6.4942e-03,  7.4746e-02,\n",
              "           9.1743e-02,  2.8841e-02, -1.4908e-02, -7.4951e-02,  6.2403e-03,\n",
              "           2.7113e-02, -5.0571e-02,  1.4710e-02,  6.0440e-02, -7.6236e-02,\n",
              "           9.0964e-02,  4.2240e-02, -6.1111e-03, -2.5226e-02,  3.2602e-02,\n",
              "          -7.5661e-02,  8.3860e-03, -4.4427e-02, -6.7322e-02,  4.4364e-02,\n",
              "           5.9016e-02,  8.1961e-02,  3.3115e-02, -4.0448e-02, -7.3765e-02,\n",
              "           9.2396e-02, -4.5676e-02,  9.7899e-02,  3.1652e-02, -9.1355e-03,\n",
              "          -7.1571e-02, -1.3682e-02,  7.2462e-02, -4.6750e-02,  6.0865e-02,\n",
              "          -4.6380e-02, -8.9269e-02,  6.8284e-02,  3.6599e-02, -4.0685e-02,\n",
              "           5.8471e-02,  7.1903e-02,  4.0558e-02,  8.0305e-02,  8.1107e-02,\n",
              "          -8.6739e-02, -9.7542e-02,  6.3673e-02, -9.2271e-02, -1.9315e-02],\n",
              "         [-4.6198e-02, -3.2290e-02, -3.0544e-02, -9.5010e-02,  3.8223e-02,\n",
              "           5.5294e-02,  4.9098e-02,  7.6608e-02, -8.0285e-02,  6.2018e-02,\n",
              "          -5.3086e-02,  3.0058e-02, -2.2902e-03, -6.1048e-03, -4.9441e-02,\n",
              "          -5.9305e-02,  3.3375e-03,  3.5207e-02,  6.2350e-02, -4.4059e-02,\n",
              "          -8.4517e-04, -8.6304e-02,  5.4937e-02, -6.0786e-02,  3.9300e-02,\n",
              "          -7.0746e-03,  7.5575e-03,  1.6690e-02,  9.7747e-02, -9.1711e-02,\n",
              "           6.9647e-02, -4.7292e-02,  6.8328e-02,  7.7011e-02, -1.1921e-02,\n",
              "           8.2020e-02,  8.4751e-02,  3.6825e-03,  9.3275e-02, -2.9666e-02,\n",
              "          -9.1267e-02, -8.1389e-02,  3.3100e-02,  6.4213e-02,  8.2357e-02,\n",
              "           5.5568e-02,  6.8318e-02, -1.3603e-02,  2.9641e-04,  8.3104e-02,\n",
              "          -2.4244e-02,  4.1599e-02, -2.7592e-02, -4.9556e-02,  5.1370e-02,\n",
              "          -9.9622e-02,  4.8358e-02, -3.7282e-04, -7.3551e-03, -6.0729e-02,\n",
              "           9.5725e-02,  1.6859e-02,  1.2552e-02,  8.2822e-02, -5.8674e-03,\n",
              "           6.0212e-02,  9.1049e-02, -4.2251e-02,  9.4969e-04,  9.7892e-02,\n",
              "          -5.9672e-02,  6.4663e-02, -6.1649e-02, -7.8423e-02, -9.9728e-02,\n",
              "           1.5248e-02,  7.8801e-03,  4.2608e-03,  1.0335e-02, -9.8835e-02,\n",
              "          -2.1486e-02, -5.8640e-03, -5.1451e-02,  2.8623e-02,  7.4678e-02,\n",
              "           5.7444e-02, -9.5587e-04,  9.2813e-02, -8.9171e-02,  6.2987e-02,\n",
              "           4.5349e-02,  3.2147e-02,  6.9089e-02,  9.8212e-02,  9.4147e-02,\n",
              "          -9.4832e-02,  8.1579e-02, -7.1328e-02, -1.0579e-02,  2.5450e-02],\n",
              "         [-2.0891e-02, -6.0200e-03,  2.5615e-02, -1.7990e-02, -4.4823e-02,\n",
              "           2.3133e-02,  2.7158e-02, -2.7425e-02, -8.8589e-02,  5.5512e-03,\n",
              "          -6.2966e-02,  8.8285e-02,  1.2188e-02,  3.6578e-02,  6.4057e-02,\n",
              "           2.6910e-02,  8.8557e-02, -4.2429e-02, -4.2300e-02, -8.4609e-02,\n",
              "           7.7359e-02,  4.4733e-02, -5.5614e-03,  2.1624e-02,  4.7043e-02,\n",
              "          -8.8673e-02,  1.0857e-03, -9.5129e-02,  6.8912e-02, -2.1803e-02,\n",
              "           2.6299e-02, -5.5845e-02,  2.7732e-02, -1.0555e-02, -4.8043e-02,\n",
              "          -5.9004e-02,  7.6596e-02, -6.0483e-02, -7.3551e-02, -6.5031e-02,\n",
              "           2.0799e-02, -7.2074e-02, -4.2507e-02,  5.0316e-02,  7.6159e-02,\n",
              "           3.6825e-02,  1.6714e-02, -9.0212e-02,  1.8672e-02,  2.7955e-02,\n",
              "          -1.2733e-02, -4.5933e-02,  5.4120e-02, -1.3092e-02, -8.3096e-02,\n",
              "           7.8286e-02,  8.5046e-02,  8.6231e-02,  2.0992e-02,  1.5767e-02,\n",
              "          -3.1872e-02,  5.8948e-02, -1.4357e-02,  5.2033e-02,  3.7963e-02,\n",
              "           5.1407e-02, -7.3431e-02,  5.3929e-02,  2.0105e-02,  9.9624e-02,\n",
              "          -1.7876e-02,  6.0013e-02, -5.3841e-02, -3.8886e-02, -3.2458e-02,\n",
              "           2.0245e-02, -1.2786e-02, -7.3796e-02, -5.1257e-02, -9.3105e-02,\n",
              "           5.3524e-02,  2.9062e-02,  6.2409e-02,  1.5023e-02,  5.3282e-02,\n",
              "           6.6994e-02,  5.3158e-02, -8.6123e-02, -8.5278e-02,  6.5582e-02,\n",
              "          -4.3465e-02,  4.4255e-02, -4.5839e-02,  5.9404e-02,  6.6008e-02,\n",
              "           6.7964e-02, -2.4449e-02, -7.3628e-02,  8.2040e-02,  7.7660e-02],\n",
              "         [-9.8090e-02,  2.3876e-02, -8.2799e-02, -7.0161e-02,  1.3677e-02,\n",
              "           6.1290e-02, -4.8976e-04, -2.8927e-03,  6.0759e-02, -8.4032e-03,\n",
              "          -3.1913e-02,  7.0797e-02,  8.9694e-02,  7.0325e-02,  6.5696e-02,\n",
              "          -1.5075e-02, -6.6608e-02,  7.0226e-02, -9.6515e-02, -1.0970e-02,\n",
              "          -7.4666e-02,  2.0471e-03,  7.9205e-02, -5.1160e-02, -8.5243e-02,\n",
              "          -8.7755e-02, -4.8625e-02,  6.3410e-02,  4.8973e-02, -3.0467e-02,\n",
              "           5.1149e-02, -6.8542e-04, -5.4671e-02,  7.0579e-02, -1.1052e-02,\n",
              "           3.7288e-02,  1.3069e-03,  6.5359e-02, -4.5996e-02,  1.3190e-03,\n",
              "           7.8334e-02, -3.5620e-02, -9.9353e-02, -3.0465e-03, -6.0722e-02,\n",
              "          -9.8107e-02, -3.9692e-02,  9.0779e-02, -2.8267e-02, -9.7357e-02,\n",
              "           6.7107e-02,  1.4171e-02,  6.0091e-02, -4.2909e-03,  6.2227e-02,\n",
              "           2.0596e-02, -9.5249e-03,  7.1651e-02, -9.0290e-02,  5.0313e-02,\n",
              "           7.2475e-02, -7.2112e-02,  8.7810e-02, -8.1037e-02, -2.8049e-02,\n",
              "           7.4033e-02, -6.5308e-02,  9.4824e-02, -7.8563e-03,  2.1731e-02,\n",
              "           5.2775e-02,  3.0641e-03, -5.3218e-02,  6.5832e-02,  6.4242e-03,\n",
              "           7.5386e-02,  9.0389e-02,  3.9998e-02, -7.2098e-03,  4.1883e-02,\n",
              "          -6.8180e-03, -3.5155e-02,  8.9635e-02, -3.1142e-02,  5.8973e-02,\n",
              "           9.9291e-02, -6.8097e-02, -7.9742e-02, -6.8372e-02,  7.2811e-02,\n",
              "           9.2736e-02, -5.7095e-02,  3.8777e-02, -2.4662e-03, -4.7575e-02,\n",
              "          -4.8032e-02, -2.5639e-02,  7.4412e-02, -8.6548e-02, -4.4684e-02]],\n",
              "        device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0935, -0.0211, -0.0509,  0.0983,  0.0549,  0.0581, -0.0937, -0.0555,\n",
              "          0.0050, -0.0360], device='cuda:0', requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "mnist_net = mnist_net.to(device)\n",
        "list(mnist_net.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "00_2j2igpS3o"
      },
      "outputs": [],
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wZtqiGvfpS3r",
        "outputId": "d2070660-1ad6-4eb6-e0fe-b7ad32e281c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9073)\n",
            "tensor(0.9177)\n",
            "tensor(0.9235)\n",
            "tensor(0.9172)\n",
            "tensor(0.9218)\n",
            "tensor(0.9247)\n",
            "tensor(0.9289)\n",
            "tensor(0.9262)\n",
            "tensor(0.9311)\n",
            "tensor(0.9314)\n",
            "tensor(0.9388)\n",
            "tensor(0.9284)\n",
            "tensor(0.9312)\n",
            "tensor(0.9383)\n",
            "tensor(0.9374)\n",
            "tensor(0.9346)\n",
            "tensor(0.9401)\n",
            "tensor(0.9434)\n",
            "tensor(0.9426)\n",
            "tensor(0.9390)\n",
            "tensor(0.9438)\n",
            "tensor(0.9434)\n",
            "tensor(0.9388)\n",
            "tensor(0.9387)\n",
            "tensor(0.9411)\n",
            "tensor(0.9394)\n",
            "tensor(0.9432)\n",
            "tensor(0.9444)\n",
            "tensor(0.9426)\n",
            "tensor(0.9484)\n",
            "tensor(0.9469)\n",
            "tensor(0.9465)\n",
            "tensor(0.9448)\n",
            "tensor(0.9440)\n",
            "tensor(0.9486)\n",
            "tensor(0.9483)\n",
            "tensor(0.9492)\n",
            "tensor(0.9490)\n",
            "tensor(0.9456)\n",
            "tensor(0.9485)\n",
            "tensor(0.9446)\n",
            "tensor(0.9490)\n",
            "tensor(0.9519)\n",
            "tensor(0.9460)\n",
            "tensor(0.9495)\n",
            "tensor(0.9533)\n",
            "tensor(0.9519)\n",
            "tensor(0.9503)\n",
            "tensor(0.9528)\n",
            "tensor(0.9543)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-775b543f4360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_size = 100\n",
        "\n",
        "test_accuracy_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "X_test = X_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "for epoch in range(10000):\n",
        "    order = np.random.permutation(len(X_train))\n",
        "    \n",
        "    for start_index in range(0, len(X_train), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        batch_indexes = order[start_index:start_index+batch_size]\n",
        "        \n",
        "        X_batch = X_train[batch_indexes].to(device)\n",
        "        y_batch = y_train[batch_indexes].to(device)\n",
        "        \n",
        "        preds = mnist_net.forward(X_batch) \n",
        "        \n",
        "        loss_value = loss(preds, y_batch)\n",
        "        loss_value.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "    test_preds = mnist_net.forward(X_test)\n",
        "    test_loss_history.append(loss(test_preds, y_test).cpu())\n",
        "    \n",
        "    accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().cpu()\n",
        "    test_accuracy_history.append(accuracy)\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "kLnumX3SpS3u",
        "outputId": "856359cd-1a39-494c-9a95-dce7483372d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d32aea47af91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plt.plot(test_accuracy_history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2767\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2768\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \"\"\"\n\u001b[1;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plt.plot(test_accuracy_history)\n",
        "plt.plot(test_loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I) График loss для train и validation:\n",
        "\n",
        "\n",
        "\n",
        "1. Построил графики до 200 эпох.\n",
        "    - test loss - как было в исходном примере\n",
        "    - train loss (last batch in an epoch) - для каждой эпохи берется значение loss на последнем минибатче - поэтому график получился очень негладкий\n",
        "    - train loss (evarage during each epoch) - среднее значения loss функции по всем минибатчам для каждой эпохи\n",
        "    \n",
        "2. Как видно из графика - train loss и test loss выходят на плато примерно после 150 эпохи. Test loss не начинает расти, можно сделать вывод, что переобучения нет.\n",
        "\n",
        "II) Ведет ли увеличение количества эпох (40 эпох -> 200 эпох) к улучшению метрик на валидации?\n",
        "\n",
        "\n",
        "\n",
        "На графике видно, что увеличение количества эпох с 40 до 200 приводит к улучшению метрики на валидации с 0.95 до 0.964\n",
        "\n",
        "III) Замерьте время вычисления 100 эпох на CPU и на GPU. Какое ускорение вы наблюдаете?\n",
        "\n",
        "На первом этапе я замерил скорость обучения на CPU и GPU, использую batch_size 100, получилось, что обучение на GPU проходит в 4,5 раза быстрее. Этого показалось мало и решил поэксперементировать с обучением при разных batch_size, чтобы получше использовать возможности GPU, ниже на графике показан результат (на графике по оси x - показаны n, где 2^n - размер batch_size):\n",
        "\n",
        "\n",
        "\n",
        "Итого преимущество GPU при большом batch_size - в 8 раз, предполагаю, что при росте размера задачи - разрыв будет увеличиваться.\n",
        "\n",
        "IV) Замедляет ли torch.backends.cudnn.deterministic = True обучение на практике? Если да, то насколько?\n",
        "\n",
        "Попробовал замерить скорость с GPU и torch.backends.cudnn.deterministic =  False, аналогично предыдущему эксперименту. Не заметил значимых изменений в быстройдействии, даже немного быстрее с True:\n",
        "\n",
        "torch.backends.cudnn.deterministic = False\n",
        "\n",
        "batch_size = 64 Time elapsed: 75.21s\n",
        "batch_size = 128 Time elapsed: 39.38s\n",
        "batch_size = 256 Time elapsed: 21.76s\n",
        "batch_size = 512 Time elapsed: 12.95s\n",
        "batch_size = 1024 Time elapsed: 8.85s\n",
        "batch_size = 2048 Time elapsed: 6.78s\n",
        "batch_size = 4096 Time elapsed: 5.71s\n",
        "batch_size = 8192 Time elapsed: 5.19s\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch_size = 64 Time elapsed: 74.92s\n",
        "batch_size = 128 Time elapsed: 39.39s\n",
        "batch_size = 256 Time elapsed: 21.55s\n",
        "batch_size = 512 Time elapsed: 12.84s\n",
        "batch_size = 1024 Time elapsed: 8.72s\n",
        "batch_size = 2048 Time elapsed: 6.68s\n",
        "batch_size = 4096 Time elapsed: 5.67s\n",
        "batch_size = 8192 Time elapsed: 5.20s\n",
        "V) Попробуйте разные методы градиентного спуска, которые были в лекции.\n",
        "\n",
        "batch_size = 8192, обучение производил на GPU по 5 раз для каждого метода, на графике показана зависимость test accuracy от эпохи (графики усредненные по 5 экспериментам для каждого метода):\n",
        "\n",
        "\n",
        "\n",
        "Видим,  что Adam и RMSprop показывают наилучшую accuracy после 200 эпох.\n",
        "\n",
        "Accuracy после 200 эпох (средняя по 5 экспериментам):\n",
        "\n",
        "SGD - 0.9193\n",
        "Rprop - 0.9271\n",
        "RMSprop - 0.9616\n",
        "Adam - 0.9596"
      ],
      "metadata": {
        "id": "NcvtYOw2Erhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IKamYOTbEoky"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8CVufB3Ep8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lesson 5 Digits Recognition Video.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}